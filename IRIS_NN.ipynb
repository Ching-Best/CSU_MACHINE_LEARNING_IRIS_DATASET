{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0843, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7887, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6856, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6293, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6075, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5971, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5911, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5872, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5845, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5827, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5814, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5804, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5796, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5790, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5785, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5780, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5776, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5772, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5768, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5765, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5762, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5759, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5756, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5753, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5750, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5747, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5744, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5741, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5738, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5735, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5732, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5729, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5726, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5724, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5721, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5719, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5716, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5714, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5711, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5709, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5706, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5704, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5701, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5699, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5697, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5695, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5692, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5690, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5688, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5686, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5684, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5682, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5680, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5678, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5677, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5675, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5673, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5671, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5670, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5668, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5667, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5665, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5664, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5662, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5661, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5659, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5658, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5657, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5656, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5654, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5653, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5652, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5651, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5650, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5649, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5647, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5646, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5645, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5644, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5643, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5642, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5641, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5640, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5640, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5639, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5638, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5637, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5636, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5635, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5635, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5634, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5633, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5632, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5632, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5631, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5630, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5629, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5631, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5628, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5628, grad_fn=<NllLossBackward>)\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Tensors likes ndarray in numpy, but I could use the GPU to accelerate computing\n",
    "\n",
    "hl = 10\n",
    "lr = 0.005 #learning rate decay\n",
    "num_epoch = 5000\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 4 input, 3 output, 10x10 square convolution kernel\n",
    "        self.fc1 = nn.Linear(4, hl)\n",
    "        self.fc2 = nn.Linear(hl, 3)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "    \n",
    "#The forward module is that \n",
    "#why “out=net(input)” , instead of “out=net.forward(input)\"\n",
    "\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    iris = load_iris()\n",
    "    x, y = shuffle(iris.data,iris.target)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=33)\n",
    "    #split set\n",
    "    net = Net()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # cross entropy if y ==1 return -log(yHat)\n",
    "            \n",
    "     #else return -log(1-yHat)\n",
    "\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    #the link for adam method：https://arxiv.org/pdf/1412.6980.pdf\n",
    "    # train\n",
    "    for epoch in range(num_epoch):\n",
    "        x = torch.Tensor(x_train).float()\n",
    "        y = torch.Tensor(y_train).long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = net(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 50 is 0:\n",
    "            print(loss) # cross entropy if y ==1 return -log(yHat)\n",
    "            \n",
    "                        #else return -log(1-yHat)\n",
    "\n",
    "\n",
    "    # test\n",
    "    x = torch.Tensor(x_test).float()\n",
    "    y = torch.Tensor(y_test).long()\n",
    "    y_pred = net(x)\n",
    "    _, predicted = torch.max(y_pred, 1)\n",
    "\n",
    "    acc = torch.sum(y == predicted).numpy() / len(x_test)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
